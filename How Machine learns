>> error function/loss function cost or loss functions or error function. The more general scenario is to define an objective function first, which you want to optimize. This objective function could be to

- maximize the posterior probabilities (e.g., naive Bayes)
- maximize a fitness function (genetic programming)
- maximize the total reward/value function (reinforcement learning)
- maximize information gain/minimize child node impurities (CART decision tree classification)
- minimize a mean squared error cost (or loss) function (CART, decision tree regression, linear regression, adaptive linear neurons, ...
- maximize log-likelihood or minimize cross-entropy loss (or cost) function
- minimize hinge loss (support vector machine)

>> Current Deep learning supports several layer types (fully connected, convolutional, max-pooling, softmax which is generelized version of sigmoid).
>> There are many activation functions (sigmoid, tanh, and rectified linear units or ReLU).
